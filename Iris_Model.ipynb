{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Iris_Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLJ3OLxpFWqXEkI4kokWev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sean-condie/from_colaboratory/blob/main/Iris_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPaF1lMId8AY"
      },
      "source": [
        "A predictive model of flower type using the classic \"iris\" dataset (addapted from google lessons)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3kRGKvFd7UL"
      },
      "source": [
        "!pip install -q sklearn #not included by default"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApvI91x1hVs3"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOs3AEVvhbbd"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_OOcp99iYTo",
        "outputId": "930bfc44-d778-4f8e-d4a2-97bdab8d4893"
      },
      "source": [
        "#import some data\n",
        "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') #training\n",
        "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') #testing\n",
        "#isolate the dependant variable we will be testing for from the dataframe\n",
        "y_train = dftrain.pop('survived')\n",
        "y_eval = dfeval.pop('survived')\n",
        "\n",
        "#diferentiate categorical and numeric data\n",
        "CATAGORICAL_DATA = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck', 'embark_town', 'alone']\n",
        "NUMERICAL_DATA = ['age', 'fare']\n",
        "\n",
        "#convert all columns to numeric\n",
        "feature_columns = []\n",
        "for feature_name in CATAGORICAL_DATA:\n",
        "  vocabulary = dftrain[feature_name].unique() #collect all the unique values within the specified column\n",
        "  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
        "\n",
        "for feature_name in NUMERICAL_DATA:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
        "\n",
        "print(feature_columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VocabularyListCategoricalColumn(key='sex', vocabulary_list=('male', 'female'), dtype=tf.string, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='n_siblings_spouses', vocabulary_list=(1, 0, 3, 4, 2, 5, 8), dtype=tf.int64, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='parch', vocabulary_list=(0, 1, 2, 5, 3, 4), dtype=tf.int64, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='class', vocabulary_list=('Third', 'First', 'Second'), dtype=tf.string, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='deck', vocabulary_list=('unknown', 'C', 'G', 'A', 'B', 'D', 'F', 'E'), dtype=tf.string, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='embark_town', vocabulary_list=('Southampton', 'Cherbourg', 'Queenstown', 'unknown'), dtype=tf.string, default_value=-1, num_oov_buckets=0), VocabularyListCategoricalColumn(key='alone', vocabulary_list=('n', 'y'), dtype=tf.string, default_value=-1, num_oov_buckets=0), NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='fare', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvRp_iXGWi6O"
      },
      "source": [
        "Batches: A subset of the entire dataset\n",
        "\n",
        "Epochs: Number of times each batch is seen\n",
        "\n",
        "Input Function: Meant to set up the data as a tf.data.Dataset object, the main object we will work with.\n",
        "\n",
        "*recall that \"label\" is the value that we are predicting/training for\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGAjNUasdrAx",
        "outputId": "25ad247c-8b12-4188-e082-9a39caae77b8"
      },
      "source": [
        "#what is the behaviour of sending a dataset to a dict object?\n",
        "df_test = dict(dftrain)\n",
        "#each row has a list of keys taken from the column names\n",
        "print(df_test['sex'][50]) #select the sex of the 50th row, as an example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "male\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWBH8C4jW8qc"
      },
      "source": [
        "#create the input function\n",
        "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32): \n",
        "  def input_function(): #this is the function that will be returned\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df)) #create the dataset object with the data and label provided\n",
        "    if shuffle: #this can be turned off by the user if shuffle is set to false in the calling statement\n",
        "      ds = ds.shuffle(1000) #randomize the data\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    return ds\n",
        "  return input_function\n",
        "\n",
        "train_input_fn = make_input_fn(dftrain, y_train)\n",
        "eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBZdSB02gt1B"
      },
      "source": [
        "We are now ready to make the model using linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72XJY_BOgtZr",
        "outputId": "e1e33ae7-eb81-459e-db08-6b5b0b85f136"
      },
      "source": [
        "#create the model\n",
        "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\n",
        "#train the model\n",
        "linear_est.train(train_input_fn)\n",
        "#evaluate the model\n",
        "result = linear_est.evaluate(eval_input_fn)\n",
        "\n",
        "clear_output() \n",
        "print(result['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7386364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ariaLkxnhLPI"
      },
      "source": [
        "We can look at the result which is a dict of result values with key names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs961k_RhOcT",
        "outputId": "0d8b0625-e2ed-4bdf-e75a-b21f7a6d5191"
      },
      "source": [
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.7386364, 'accuracy_baseline': 0.625, 'auc': 0.8345577, 'auc_precision_recall': 0.78871053, 'average_loss': 0.47621182, 'label/mean': 0.375, 'loss': 0.4686921, 'precision': 0.64705884, 'prediction/mean': 0.3962939, 'recall': 0.6666667, 'global_step': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21ncxVRRkfGm"
      },
      "source": [
        "\n",
        "END OF LINEAR REGRESSION\n",
        "\n",
        "---\n",
        "\n",
        "START OF CLASSIFICATION\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpN48NcLkg9N",
        "outputId": "15b3db8a-c18c-4389-b4a1-af24b153421c"
      },
      "source": [
        "#import the data\n",
        "train_path = tf.keras.utils.get_file(\"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
        "test_path = tf.keras.utils.get_file(\"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
        "\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "\n",
        "#look at the data to see what is going on\n",
        "print(train.head)\n",
        "\n",
        "#column names are absent so we must create some names from the documentation\n",
        "COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
        "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
        "\n",
        "#reload the data without a header and manualy assigned column names\n",
        "train = pd.read_csv(train_path, names=COLUMN_NAMES, header=0)\n",
        "test = pd.read_csv(test_path, names=COLUMN_NAMES, header=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\n",
            "16384/2194 [================================================================================================================================================================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\n",
            "16384/573 [=========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 0us/step\n",
            "<bound method NDFrame.head of      120    4  setosa  versicolor  virginica\n",
            "0    6.4  2.8     5.6         2.2          2\n",
            "1    5.0  2.3     3.3         1.0          1\n",
            "2    4.9  2.5     4.5         1.7          2\n",
            "3    4.9  3.1     1.5         0.1          0\n",
            "4    5.7  3.8     1.7         0.3          0\n",
            "..   ...  ...     ...         ...        ...\n",
            "115  5.5  2.6     4.4         1.2          1\n",
            "116  5.7  3.0     4.2         1.2          1\n",
            "117  4.4  2.9     1.4         0.2          0\n",
            "118  4.8  3.0     1.4         0.1          0\n",
            "119  5.5  2.4     3.7         1.0          1\n",
            "\n",
            "[120 rows x 5 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qG9G612pNoQ"
      },
      "source": [
        "We will be using species as the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggB4NAqtpSTx"
      },
      "source": [
        "#pop off the species from the dataframe\n",
        "train_y = train.pop('Species')\n",
        "test_y = test.pop('Species')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7uwNhlxqNOT"
      },
      "source": [
        "Create the input function to generate a Dataset object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C23DQ-KUqUGW"
      },
      "source": [
        "#create input function\n",
        "def input_fn(features,  labels, training=True, batch_size=256):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "  if training:\n",
        "    dataset = dataset.shuffle(1000).repeat()\n",
        "  \n",
        "  return dataset.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_9wzLv9rUPJ"
      },
      "source": [
        "Set up the feature columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO_csX1drTci"
      },
      "source": [
        "feature_columns = []\n",
        "for key in train.keys():\n",
        "  feature_columns.append(tf.feature_column.numeric_column(key=key))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8xRWFG6sRYk"
      },
      "source": [
        "We can now build the classifier model using a Deep Neural Network type algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri88H-77sY-H",
        "outputId": "5b3aa126-c4d9-4eb1-bad1-8dede28a1cd5"
      },
      "source": [
        "#create the model\n",
        "classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns, \n",
        "                                        hidden_units=[30,10], #the two hidden layers will have 30 and 10 nodes, respectively\n",
        "                                        n_classes=3) #the network ends by making a selection between three nodes\n",
        "\n",
        "#train the model\n",
        "classifier.train(lambda:input_fn(train, train_y, training=True), steps=5000)\n",
        "\n",
        "eval_results = classifier.evaluate(lambda:input_fn(test, test_y, training=False))\n",
        "\n",
        "clear_output()\n",
        "print(eval_results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.56666666, 'average_loss': 0.60299075, 'loss': 0.60299075, 'global_step': 5000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrRiCUqqyQB4"
      },
      "source": [
        "Now that the model is trained, we can make predictions based on the features. We will need a new input function since the labels are not being provided, they are being predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeaNIHLIyPZZ",
        "outputId": "eccbbfab-8fc8-439c-90ac-7e06cd21d218"
      },
      "source": [
        "3#prediction input function\n",
        "def input_fn(features, batch_size=256):\n",
        "  return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size) #returning a Dataset that has no labels\n",
        "\n",
        "#list of the features used to predict the species\n",
        "features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
        "#empty dictionary for the predictions\n",
        "predict = {}\n",
        "\n",
        "#prompt the userr for values\n",
        "print(\"enter the values as prompted.\")\n",
        "\n",
        "#colect the value for each feature\n",
        "for feature in features:\n",
        "  valid = False \n",
        "  while not valid:\n",
        "    value = input(feature + \": \")\n",
        "    if value.isdigit(): #make sure it is a digit\n",
        "      valid = True #exit the while loop if it is a valid digit\n",
        "  \n",
        "  #add the feature value to the dictionary\n",
        "  predict[feature] = [float(value)]\n",
        "\n",
        "#predict the species based on our entered values\n",
        "predictions = classifier.predict(lambda:input_fn(predict))\n",
        "\n",
        "#look at the results\n",
        "for result in predictions:\n",
        "  class_id = result['class_ids'][0] #class_ids contains all predicted results. In this case it will only be one result for species, but it must be indexed regardless.\n",
        "  probability = result['probabilities'][class_id] #probabilities contains a prediction value for each possible label (classification), index it to the highest value which is stored in class_ids\n",
        "\n",
        "  #print the predicted value\n",
        "  print('prediction is \"{}\" ({:.1f}%)'.format(SPECIES[class_id], 100 * probability))\n",
        "  \n",
        "  #print the probabilities of the other species\n",
        "  for spec in result['all_class_ids']:\n",
        "    if spec != class_id: #exclude the already predicted species\n",
        "      probability = result['probabilities'][spec]\n",
        "      print('the probability of it being \"{}\" is {:.1f}%'.format(SPECIES[spec], 100 * probability))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the values as prompted.\n",
            "SepalLength: 10\n",
            "SepalWidth: 15\n",
            "PetalLength: 3\n",
            "PetalWidth: 5\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmprn03d0ae/model.ckpt-5000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "prediction is \"Setosa\" (99.7%)\n",
            "the probability of it being \"Versicolor\" is 0.1%\n",
            "the probability of it being \"Virginica\" is 0.2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmNxTpeM8Jc6"
      },
      "source": [
        "END OF CATEGORIZATION\n",
        "___________________________________"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyWP85Ta8JLp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}